from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import asyncio
from concurrent.futures import ThreadPoolExecutor
import openai
import os
import json
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Get OpenAI API key from environment variable
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("OpenAI API key not found. Set OPENAI_API_KEY environment variable.")

# Initialize OpenAI client
client = openai.OpenAI(api_key=OPENAI_API_KEY)

app = FastAPI()

# Add CORS middleware to allow requests from the frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Adjust in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class IssueRequest(BaseModel):
    title: str
    content: str
    repo_url: str

class SolutionResponse(BaseModel):
    solution: str
    related_files: list[str]

def get_issue_dependency_graph(repo_url):
    """Get issue-specific dependency graph generated by next_context.py"""
    # The analysis is stored in the project root directory
    current_dir = os.path.dirname(os.path.abspath(__file__))
    json_path = os.path.join(current_dir, "nextjs_dependency_graph.json")
    
    dependency_graph = None
    
    # Read JSON analysis if available
    if os.path.exists(json_path):
        try:
            with open(json_path, 'r') as f:
                dependency_graph = json.load(f)
        except Exception as e:
            print(f"Error reading dependency graph file: {e}")
            return None
    else:
        print(f"Dependency graph file not found at: {json_path}")
        return None
    
    return dependency_graph

def format_relevant_files_for_context(dependency_graph):
    """Format the relevant files data for inclusion in the prompt context"""
    if not dependency_graph or "relevant_files" not in dependency_graph:
        return "No relevant files found in dependency graph."
    
    relevant_files = dependency_graph["relevant_files"]
    
    # Build a formatted context string
    context = "Relevant Files Analysis:\n\n"
    
    for file_path, file_info in relevant_files.items():
        context += f"File: {file_path}\n"
        
        # Add imports information
        if "imports" in file_info and file_info["imports"]:
            context += "Imports:\n"
            for imp in file_info["imports"]:
                if imp["type"] == "internal":
                    context += f"  - {imp['path']} (resolved to {imp['resolved']})\n"
            context += "\n"
        
        # Add "imported by" information
        if "imported_by" in file_info and file_info["imported_by"]:
            context += "Imported by:\n"
            for imp_by in file_info["imported_by"]:
                context += f"  - {imp_by}\n"
            context += "\n"
        
        # Add relevant code snippets
        if "relevant_content" in file_info and file_info["relevant_content"]:
            context += "Relevant code snippets:\n```\n"
            context += file_info["relevant_content"]
            context += "\n```\n\n"
        
        context += "---\n\n"
    
    return context

@app.post("/generate-solution", response_model=SolutionResponse)
async def generate_solution(request: Request, issue: IssueRequest):
    try:
        # Get issue-specific dependency graph
        dependency_graph = get_issue_dependency_graph(issue.repo_url)
        
        # Extract list of related files for the response
        related_files = []
        if dependency_graph and "relevant_files" in dependency_graph:
            related_files = list(dependency_graph["relevant_files"].keys())
        
        # Format dependency graph for context
        repo_context = ""
        if dependency_graph:
            formatted_context = format_relevant_files_for_context(dependency_graph)
            repo_context = f"""
            Issue-Specific Repository Analysis:
            {formatted_context}
            """
        
        # Build the prompt
        prompt = f"""
        Repository: {issue.repo_url}
        Issue Title: {issue.title}
        Issue Description:
        {issue.content}
        
        {repo_context}
        
        Please provide a comprehensive solution for this issue. Include:
        1. Analysis of the problem
        2. Suggested approach
        3. Code snippets or examples if applicable
        4. Detailed implementation steps
        5. References or resources that might be helpful
        
        Based on the repository structure and relevant files shown above, provide specific guidance on how to solve the issue.
        """
        
        # Call OpenAI API
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as pool:
            future = loop.run_in_executor(pool, lambda: client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant that provides solutions to GitHub issues. Use the repository structure information to provide detailed, accurate, and context-aware solutions."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1500,
            ))

            while not future.done():
                if await request.is_disconnected():
                    raise HTTPException(status_code=499, detail="Client disconnected during solution generation")
                await asyncio.sleep(0.2)  # Polling interval

            response = await future
            return {
                "solution": response.choices[0].message.content,
                "related_files": related_files
            }
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)